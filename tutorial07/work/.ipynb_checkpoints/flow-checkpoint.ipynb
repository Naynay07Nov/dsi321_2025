{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d2b421-b3cc-4440-b0c7-654039057708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/site-packages (19.0.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (2025.3.2)\n",
      "Requirement already satisfied: s3fs in /usr/local/lib/python3.11/site-packages (2025.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.11/site-packages (from s3fs) (2.22.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/site-packages (from s3fs) (3.11.18)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.37.4,>=1.37.2 in /usr/local/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.37.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.4.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/site-packages (from botocore<1.37.4,>=1.37.2->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.2.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas pyarrow fsspec s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43317ff8-fa63-4ae7-9f8b-963bf9c26587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement lakefs-s3 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for lakefs-s3\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lakefs-s3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72d7818c-7931-4e06-a76c-6b90abd7a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from prefect import flow, task # Prefect flow and task decorators\n",
    "\n",
    "# API endpoint and parameters\n",
    "WEATHER_ENDPOINT = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "API_KEY = \"1de9eaf371dc114af656a95cd26febd8\"  # Replace with your actual API key\n",
    "\n",
    "\n",
    "@task\n",
    "def get_weather_data(location_context={'location':None, 'province':None, 'lat':None, 'lon':None}):\n",
    "    # API endpoint and parameters\n",
    "    WEATHER_ENDPOINT = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "    API_KEY = \"a8830679af88ae345ed7fb6aac741e34\"  # Replace with your actual API key\n",
    "    location = location_context['location']\n",
    "    province = location_context['province']\n",
    "    \n",
    "    params = {\n",
    "        \"lat\": location_context['lat'],\n",
    "        \"lon\": location_context['lon'],\n",
    "        \"appid\": API_KEY,\n",
    "        \"units\": \"metric\"\n",
    "    }\n",
    "    try:\n",
    "        # Make API request\n",
    "        response = requests.get(WEATHER_ENDPOINT, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        data = response.json()\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        dt = datetime.now()\n",
    "        thai_tz = pytz.timezone('Asia/Bangkok')\n",
    "        created_at = dt.replace(tzinfo=thai_tz)\n",
    "\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Create dictionary with required fields\n",
    "        weather_dict = {\n",
    "            'timestamp': timestamp,\n",
    "            'year': timestamp.year,\n",
    "            'month': timestamp.month,\n",
    "            'day': timestamp.day,\n",
    "            'hour': timestamp.hour,\n",
    "            'minute': timestamp.minute,\n",
    "            'created_at': created_at,\n",
    "            'province': province,  # เลือก\n",
    "            'location_name': location,  # ชื่อสถาน\n",
    "            'api_location': data['name'],  # ชื่อสถานจาก API\n",
    "            'weather_main': data['weather'][0]['main'],\n",
    "            'weather_description': data['weather'][0]['description'],\n",
    "            'main.temp': data['main']['temp'],\n",
    "            # Adding humidity\n",
    "            'main.humidity': data['main']['humidity'],\n",
    "            # Adding wind speed\n",
    "            'wind.speed': data['wind']['speed'],\n",
    "            # Adding precipitation (rain or snow)\n",
    "            'precipitation': get_precipitation(data)\n",
    "        }\n",
    "        \n",
    "        return weather_dict\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing data: Missing key {e}\")\n",
    "        return None\n",
    "\n",
    "# Helper function to extract precipitation data\n",
    "def get_precipitation(data):\n",
    "    # Check for rain data (last 1 hour)\n",
    "    if 'rain' in data and '1h' in data['rain']:\n",
    "        return data['rain']['1h']\n",
    "    # Check for snow data (last 1 hour)\n",
    "    elif 'snow' in data and '1h' in data['snow']:\n",
    "        return data['snow']['1h']\n",
    "    # No precipitation\n",
    "    else:\n",
    "        return 0.0\n",
    "@flow(name=\"main-flow\", log_prints=True)\n",
    "def main_flow(parameters={}):\n",
    "    locations = {\n",
    "        \n",
    "    \"Satitram Alumni\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.752916,\n",
    "        \"lon\": 100.618616\n",
    "    },\n",
    "    # เล่มสถานใหม่\n",
    "    \"Ramkhamhaeng University\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7552,\n",
    "        \"lon\": 100.6201\n",
    "    },\n",
    "    \"Rajamangala National Stadium\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7627,\n",
    "        \"lon\": 100.6200\n",
    "    },\n",
    "    \"Ramkhamhaeng Hospital\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7485,\n",
    "        \"lon\": 100.6265\n",
    "    },\n",
    "    \"Hua Mak Police Station\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7500,\n",
    "        \"lon\": 100.6200\n",
    "    },\n",
    "    \"Bang Kapi District Office\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7640,\n",
    "        \"lon\": 100.6440\n",
    "    },\n",
    "    \"The Mall Bangkapi\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7650,\n",
    "        \"lon\": 100.6430\n",
    "    },\n",
    "    \"Tawanna Market\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7655,\n",
    "        \"lon\": 100.6425\n",
    "    },\n",
    "    \"Big C Huamark\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7445,\n",
    "        \"lon\": 100.6205\n",
    "    },\n",
    "    \"Makro Ladprao\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7940,\n",
    "        \"lon\": 100.6110\n",
    "    },\n",
    "    \"Siam Paragon\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7458,\n",
    "        \"lon\": 100.5343\n",
    "    },\n",
    "    \"ICONSIAM\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7292,\n",
    "        \"lon\": 100.5103\n",
    "    },\n",
    "    \"HomePro Rama 9\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7430,\n",
    "        \"lon\": 100.6155\n",
    "    },\n",
    "    \"The Mall Ramkhamhaeng\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7526,\n",
    "        \"lon\": 100.6095\n",
    "    },\n",
    "    \"Healthy Park\": {\n",
    "        \"province\": \"Bangkok\",\n",
    "        \"lat\": 13.7565,\n",
    "        \"lon\": 100.6275\n",
    "    }\n",
    "}\n",
    "    \n",
    "    df=pd.DataFrame([get_weather_data(\n",
    "        {\n",
    "            'location': location,\n",
    "            'province': locations[location]['province'],\n",
    "            'lat': locations[location]['lat'],\n",
    "            'lon': locations[location]['lon'],\n",
    "        }\n",
    "    ) for location in list(locations.keys())])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fa0be2-9d17-46a7-9065-4507629fd910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count  Dtype                       \n",
      "---  ------               --------------  -----                       \n",
      " 0   timestamp            2 non-null      datetime64[ns]              \n",
      " 1   year                 2 non-null      int64                       \n",
      " 2   month                2 non-null      int64                       \n",
      " 3   day                  2 non-null      int64                       \n",
      " 4   hour                 2 non-null      int64                       \n",
      " 5   minute               2 non-null      int64                       \n",
      " 6   created_at           2 non-null      datetime64[ns, Asia/Bangkok]\n",
      " 7   requested_province   2 non-null      object                      \n",
      " 8   location             2 non-null      object                      \n",
      " 9   weather_main         2 non-null      object                      \n",
      " 10  weather_description  2 non-null      object                      \n",
      " 11  main.temp            2 non-null      float64                     \n",
      "dtypes: datetime64[ns, Asia/Bangkok](1), datetime64[ns](1), float64(1), int64(5), object(4)\n",
      "memory usage: 324.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>created_at</th>\n",
       "      <th>requested_province</th>\n",
       "      <th>location</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>main.temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-12 17:50:32.293918</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>50</td>\n",
       "      <td>2025-05-12 18:08:32.263217+07:00</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Pathum Wan</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>30.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-12 17:50:32.426843</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>50</td>\n",
       "      <td>2025-05-12 18:08:32.426816+07:00</td>\n",
       "      <td>Satitram Association</td>\n",
       "      <td>Bang Kapi</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>29.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp  year  month  day  hour  minute  \\\n",
       "0 2025-05-12 17:50:32.293918  2025      5   12    17      50   \n",
       "1 2025-05-12 17:50:32.426843  2025      5   12    17      50   \n",
       "\n",
       "                        created_at    requested_province    location  \\\n",
       "0 2025-05-12 18:08:32.263217+07:00               Bangkok  Pathum Wan   \n",
       "1 2025-05-12 18:08:32.426816+07:00  Satitram Association   Bang Kapi   \n",
       "\n",
       "  weather_main weather_description  main.temp  \n",
       "0       Clouds     overcast clouds      30.22  \n",
       "1       Clouds     overcast clouds      29.59  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([get_weather_data(p) for p in list(provinces.keys())])\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2512a494-ce68-49c3-84b8-4291a9869860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lakefs-spec in /usr/local/lib/python3.11/site-packages (0.11.3)\n",
      "Requirement already satisfied: fsspec>=2023.12.0 in /usr/local/lib/python3.11/site-packages (from lakefs-spec) (2025.3.2)\n",
      "Requirement already satisfied: lakefs>=0.2.0 in /usr/local/lib/python3.11/site-packages (from lakefs-spec) (0.10.0)\n",
      "Requirement already satisfied: lakefs-sdk<2,>=1.50 in /usr/local/lib/python3.11/site-packages (from lakefs>=0.2.0->lakefs-spec) (1.54.0)\n",
      "Requirement already satisfied: pyyaml~=6.0.1 in /usr/local/lib/python3.11/site-packages (from lakefs>=0.2.0->lakefs-spec) (6.0.1)\n",
      "Requirement already satisfied: urllib3<2.1.0,>=1.25.3 in /usr/local/lib/python3.11/site-packages (from lakefs-sdk<2,>=1.50->lakefs>=0.2.0->lakefs-spec) (2.0.7)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/site-packages (from lakefs-sdk<2,>=1.50->lakefs>=0.2.0->lakefs-spec) (2.9.0.post0)\n",
      "Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.11/site-packages (from lakefs-sdk<2,>=1.50->lakefs>=0.2.0->lakefs-spec) (2.8.2)\n",
      "Requirement already satisfied: aenum in /usr/local/lib/python3.11/site-packages (from lakefs-sdk<2,>=1.50->lakefs>=0.2.0->lakefs-spec) (3.1.16)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=1.10.5->lakefs-sdk<2,>=1.50->lakefs>=0.2.0->lakefs-spec) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.11/site-packages (from pydantic>=1.10.5->lakefs-sdk<2,>=1.50->lakefs>=0.2.0->lakefs-spec) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.11/site-packages (from pydantic>=1.10.5->lakefs-sdk<2,>=1.50->lakefs>=0.2.0->lakefs-spec) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil->lakefs-sdk<2,>=1.50->lakefs>=0.2.0->lakefs-spec) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lakefs-spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdd175b-f75b-43d7-92ef-2a83237e3669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 17:50:32.503050+06:42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dt = datetime.now()\n",
    "thai_tz = pytz.timezone('Asia/Bangkok')\n",
    "dt = dt.replace(tzinfo=thai_tz)\n",
    "print(dt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56f77c2b-a1be-46c4-9537-cd7aca2c0323",
   "metadata": {},
   "outputs": [],
   "source": [
    " # lakeFS credentials from your docker-compose.yml\n",
    "ACCESS_KEY = \"access_key\"\n",
    "SECRET_KEY = \"secret_key\"\n",
    "    \n",
    "# lakeFS endpoint (running locally)\n",
    "lakefs_endpoint = \"http://lakefs-dev:8000/\"\n",
    "    \n",
    "# lakeFS repository, branch, and file path\n",
    "repo = \"weather\"\n",
    "branch = \"main\"\n",
    "path = \"weather.parquet\"\n",
    "\n",
    "# Construct the full lakeFS S3-compatible path\n",
    "lakefs_s3_path = f\"s3a://{repo}/{branch}/{path}\"\n",
    "    \n",
    "# Configure storage_options for lakeFS (S3-compatible)\n",
    "storage_options = {\n",
    "    \"key\": ACCESS_KEY,\n",
    "    \"secret\": SECRET_KEY,\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": lakefs_endpoint   \n",
    "    }   \n",
    "}\n",
    "df.to_parquet(\n",
    "    lakefs_s3_path,\n",
    "    storage_options=storage_options,    \n",
    "    partition_cols=['year','month','day','hour'],\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545df5e-b3f6-4c46-a196-7334bb0af669",
   "metadata": {},
   "source": [
    "# test read parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9703e66e-ea17-47cf-9067-4cca2fa8c2b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "weather3/main/weather.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m path_all_partition = \u001b[33m'\u001b[39m\u001b[33ms3a://weather3/main/weather.parquet\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df2=\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m    \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_all_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df2.info()\n\u001b[32m      8\u001b[39m df2.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py:274\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    268\u001b[39m     path,\n\u001b[32m    269\u001b[39m     filesystem,\n\u001b[32m    270\u001b[39m     storage_options=storage_options,\n\u001b[32m    271\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m )\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     result = pa_table.to_pandas(**to_pandas_kwargs)\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/parquet/core.py:1793\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1787\u001b[39m     warnings.warn(\n\u001b[32m   1788\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_legacy_dataset\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1790\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1813\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/parquet/core.py:1371\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[39m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partitioning == \u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1368\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1369\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/dataset.py:794\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    783\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    784\u001b[39m     schema=schema,\n\u001b[32m    785\u001b[39m     filesystem=filesystem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m     selector_ignore_prefixes=ignore_prefixes\n\u001b[32m    791\u001b[39m )\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/dataset.py:476\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    474\u001b[39m         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     fs, paths_or_selector = \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    479\u001b[39m     partitioning=partitioning,\n\u001b[32m    480\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    481\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    482\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    483\u001b[39m )\n\u001b[32m    484\u001b[39m factory = FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/dataset.py:441\u001b[39m, in \u001b[36m_ensure_single_source\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    439\u001b[39m     paths_or_selector = [path]\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[31mFileNotFoundError\u001b[39m: weather3/main/weather.parquet"
     ]
    }
   ],
   "source": [
    "path_all_partition = 's3a://weather3/main/weather.parquet'\n",
    "\n",
    "df2=pd.read_parquet(    \n",
    "    path=path_all_partition,\n",
    "    storage_options=storage_options\n",
    ")\n",
    "df2.info()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b483f8da-bc2a-4147-8684-4c5310a0472e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "weather3/main/weather.parquet/year=2025/month=4/day=10/hour=7/70cf855c5c5e4659ae01aba885c731c3-0.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m path_single_partition = \u001b[33m'\u001b[39m\u001b[33ms3a://weather3/main/weather.parquet/year=2025/month=4/day=10/hour=7/70cf855c5c5e4659ae01aba885c731c3-0.parquet\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df2=\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m    \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_single_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df2.info()\n\u001b[32m      8\u001b[39m df2.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py:274\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    268\u001b[39m     path,\n\u001b[32m    269\u001b[39m     filesystem,\n\u001b[32m    270\u001b[39m     storage_options=storage_options,\n\u001b[32m    271\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m )\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     result = pa_table.to_pandas(**to_pandas_kwargs)\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/parquet/core.py:1793\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1787\u001b[39m     warnings.warn(\n\u001b[32m   1788\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_legacy_dataset\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1790\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1813\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/parquet/core.py:1371\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[39m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partitioning == \u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1368\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1369\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/dataset.py:794\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    783\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    784\u001b[39m     schema=schema,\n\u001b[32m    785\u001b[39m     filesystem=filesystem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m     selector_ignore_prefixes=ignore_prefixes\n\u001b[32m    791\u001b[39m )\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/dataset.py:476\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    474\u001b[39m         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     fs, paths_or_selector = \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    479\u001b[39m     partitioning=partitioning,\n\u001b[32m    480\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    481\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    482\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    483\u001b[39m )\n\u001b[32m    484\u001b[39m factory = FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/dataset.py:441\u001b[39m, in \u001b[36m_ensure_single_source\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    439\u001b[39m     paths_or_selector = [path]\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[31mFileNotFoundError\u001b[39m: weather3/main/weather.parquet/year=2025/month=4/day=10/hour=7/70cf855c5c5e4659ae01aba885c731c3-0.parquet"
     ]
    }
   ],
   "source": [
    "path_single_partition = 's3a://weather3/main/weather.parquet/year=2025/month=4/day=10/hour=7/70cf855c5c5e4659ae01aba885c731c3-0.parquet'\n",
    "\n",
    "df2=pd.read_parquet(    \n",
    "    path=path_single_partition,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "df2.info()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6909c16e-e300-40b2-b9b0-2c9ecad69c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-10 07:50:44.311326+06:42\n"
     ]
    }
   ],
   "source": [
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d82d9f-46e6-4148-aa59-c402867f7529",
   "metadata": {},
   "source": [
    "# Test Duck and Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0392960c-a68b-4b92-8981-0379fd93b7b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPException",
     "evalue": "HTTP Error: Unable to connect to URL \"http://lakefs-dev:8000/weather3/main/weather.parquet\": 404 (Not Found).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 32\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Use DuckDB's read_parquet() function to read the dataset.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Assume the dataset is stored in the directory \"output_parquet_dataset\" with hive partitions:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# output_parquet_dataset/year=2025/month=02/day=17/...\u001b[39;00m\n\u001b[1;32m     18\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mINSTALL httpfs;\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mLOAD httpfs;\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124mFROM read_parquet(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3a://weather3/main/weather.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 32\u001b[0m df_duck \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdf()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuckDB Parquet Query Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m df_duck\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mHTTPException\u001b[0m: HTTP Error: Unable to connect to URL \"http://lakefs-dev:8000/weather3/main/weather.parquet\": 404 (Not Found)."
     ]
    }
   ],
   "source": [
    "# pip install duckdb\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to an in-memory DuckDB instance.\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "storage_options = {\n",
    "    \"key\": ACCESS_KEY,\n",
    "    \"secret\": SECRET_KEY,\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": lakefs_endpoint\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use DuckDB's read_parquet() function to read the dataset.\n",
    "# Assume the dataset is stored in the directory \"output_parquet_dataset\" with hive partitions:\n",
    "# output_parquet_dataset/year=2025/month=02/day=17/...\n",
    "query = \"\"\"\n",
    "INSTALL httpfs;\n",
    "LOAD httpfs;\n",
    "\n",
    "SET s3_endpoint='lakefs-dev:8000';\n",
    "SET s3_access_key_id='access_key'; \n",
    "SET s3_secret_access_key='secret_key'; \n",
    "SET s3_url_style='path';\n",
    "SET s3_use_ssl=false;\n",
    "\n",
    "SELECT *\n",
    "FROM read_parquet('s3a://weather3/main/weather.parquet')\n",
    "\"\"\"\n",
    "\n",
    "df_duck = con.execute(query).df()\n",
    "\n",
    "print(\"DuckDB Parquet Query Result:\")\n",
    "df_duck.info()\n",
    "df_duck.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d3c6a0fe-a4e2-40bc-96bc-e72445f8a352",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: pyarrow>=10.0.1 is required for PyArrow backed StringArray.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/backends.py:140\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:5376\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, engine, arrow_to_pandas, **kwargs)\u001b[0m\n\u001b[1;32m   5357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[1;32m   5358\u001b[0m         ReadParquetPyarrowFS(\n\u001b[1;32m   5359\u001b[0m             path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5372\u001b[0m         )\n\u001b[1;32m   5373\u001b[0m     )\n\u001b[1;32m   5375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[0;32m-> 5376\u001b[0m     \u001b[43mReadParquetFSSpec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalculate_divisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalculate_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5386\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5388\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_set_parquet_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5393\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5394\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5395\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/_expr.py:63\u001b[0m, in \u001b[0;36mExpr.__new__\u001b[0;34m(cls, _determ_token, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m inst\u001b[38;5;241m.\u001b[39moperands \u001b[38;5;241m=\u001b[39m [_unpack_collections(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m operands]\n\u001b[0;32m---> 63\u001b[0m _name \u001b[38;5;241m=\u001b[39m \u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _name \u001b[38;5;129;01min\u001b[39;00m Expr\u001b[38;5;241m.\u001b[39m_instances:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/dask_expr/io/parquet.py:791\u001b[0m, in \u001b[0;36mReadParquet._name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_name\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_funcname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic_token\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/dask_expr/io/parquet.py:785\u001b[0m, in \u001b[0;36mReadParquet.deterministic_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determ_token:\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# TODO: Is there an actual need to overwrite this?\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determ_token \u001b[38;5;241m=\u001b[39m _tokenize_deterministic(\n\u001b[0;32m--> 785\u001b[0m         funcname(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchecksum\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    786\u001b[0m     )\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determ_token\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/dask_expr/io/parquet.py:795\u001b[0m, in \u001b[0;36mReadParquet.checksum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchecksum\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_info\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchecksum\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/dask_expr/io/parquet.py:1399\u001b[0m, in \u001b[0;36mReadParquetFSSpec._dataset_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;66;03m# Infer meta, accounting for index and columns arguments.\u001b[39;00m\n\u001b[0;32m-> 1399\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dd_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1400\u001b[0m index \u001b[38;5;241m=\u001b[39m dataset_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:1146\u001b[0m, in \u001b[0;36mArrowDatasetEngine._create_dd_meta\u001b[0;34m(cls, dataset_info)\u001b[0m\n\u001b[1;32m   1145\u001b[0m dtype_backend \u001b[38;5;241m=\u001b[39m dataset_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1146\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arrow_table_to_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43marrow_to_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrow_to_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m index_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(meta\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:1800\u001b[0m, in \u001b[0;36mArrowDatasetEngine._arrow_table_to_pandas\u001b[0;34m(cls, arrow_table, categories, dtype_backend, convert_string, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m _kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[0;32m-> 1800\u001b[0m types_mapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_determine_type_mapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m types_mapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:1759\u001b[0m, in \u001b[0;36mArrowDatasetEngine._determine_type_mapper\u001b[0;34m(cls, dtype_backend, convert_string, **kwargs)\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_string:\n\u001b[0;32m-> 1759\u001b[0m     type_mappers\u001b[38;5;241m.\u001b[39mappend({pa\u001b[38;5;241m.\u001b[39mstring(): \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStringDtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m}\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m   1760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m PANDAS_GE_220:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/string_.py:131\u001b[0m, in \u001b[0;36mStringDtype.__init__\u001b[0;34m(self, storage)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow_numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m pa_version_under10p1:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow>=10.0.1 is required for PyArrow backed StringArray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage \u001b[38;5;241m=\u001b[39m storage\n",
      "\u001b[0;31mImportError\u001b[0m: pyarrow>=10.0.1 is required for PyArrow backed StringArray.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_all_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m  \n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/backends.py:151\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: pyarrow>=10.0.1 is required for PyArrow backed StringArray."
     ]
    }
   ],
   "source": [
    "# pip install dask\n",
    "import dask.dataframe as dd\n",
    "df2 = dd.read_parquet(\n",
    "    path=path_all_partition,\n",
    "    storage_options=storage_options,\n",
    "    dtype_backend='pyarrow'\n",
    ")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
